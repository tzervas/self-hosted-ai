# NetworkPolicy for gpu-workloads namespace
# Allows GPU inference services to be accessed from ai-services
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: gpu-workloads
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress
---
# Allow DNS egress
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-dns-egress
  namespace: gpu-workloads
spec:
  podSelector: {}
  policyTypes:
    - Egress
  egress:
    - to:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: kube-system
      ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
---
# Allow ingress from ai-services to all GPU services (Ollama, ComfyUI, audio, video)
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-ai-services-ingress
  namespace: gpu-workloads
spec:
  podSelector: {}  # All pods in gpu-workloads
  policyTypes:
    - Ingress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: ai-services
      ports:
        - protocol: TCP
          port: 11434  # Ollama GPU
        - protocol: TCP
          port: 8188   # ComfyUI
        - protocol: TCP
          port: 5004   # Audio server
        - protocol: TCP
          port: 5005   # Video server
---
# Allow ingress from automation namespace (n8n -> ComfyUI, audio, video for workflows)
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-automation-ingress
  namespace: gpu-workloads
spec:
  podSelector: {}
  policyTypes:
    - Ingress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: automation
      ports:
        - protocol: TCP
          port: 8188   # ComfyUI
        - protocol: TCP
          port: 5004   # Audio server
        - protocol: TCP
          port: 5005   # Video server
---
# Allow LAN access to Ollama GPU
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-lan-to-ollama
  namespace: gpu-workloads
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/name: ollama
  policyTypes:
    - Ingress
  ingress:
    - from:
        - ipBlock:
            cidr: 192.168.0.0/16
      ports:
        - protocol: TCP
          port: 11434
---
# Allow egress for model downloads (Ollama + ComfyUI need to pull models)
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-model-downloads
  namespace: gpu-workloads
spec:
  podSelector: {}  # All GPU workload pods can download models
  policyTypes:
    - Egress
  egress:
    # Allow HTTPS to anywhere for model downloads (HuggingFace, registry, etc.)
    - to:
        - ipBlock:
            cidr: 0.0.0.0/0
            except:
              - 10.0.0.0/8
              - 172.16.0.0/12
      ports:
        - protocol: TCP
          port: 443
