# TTS Server (Coqui XTTS-v2) - Text to Speech
# Runs on akula-prime GPU node for fast inference
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: tts-server
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "6"  # Deploy after Ollama GPU
spec:
  project: default
  source:
    repoURL: https://github.com/tzervas/self-hosted-ai.git
    targetRevision: main
    path: helm/gpu-worker
    helm:
      releaseName: tts-server
      values: |
        # Only deploy TTS (disable other components)
        ollama:
          gpu:
            enabled: false

        comfyui:
          enabled: false

        automatic1111:
          enabled: false

        audio:
          enabled: false

        video:
          enabled: false

        tts:
          enabled: true
          image:
            repository: ghcr.io/coqui-ai/tts
            tag: latest
            pullPolicy: IfNotPresent
          service:
            type: ClusterIP
            port: 5002
          env:
            - name: USE_CUDA
              value: "true"
            - name: COQUI_TOS_AGREED
              value: "1"
          nodeSelector:
            kubernetes.io/hostname: akula-prime
          tolerations:
            - key: nvidia.com/gpu
              operator: Exists
              effect: NoSchedule
          resources:
            requests:
              memory: "2Gi"
              cpu: "500m"
            limits:
              memory: "8Gi"
              cpu: "2000m"
  destination:
    server: https://kubernetes.default.svc
    namespace: gpu-workloads
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
