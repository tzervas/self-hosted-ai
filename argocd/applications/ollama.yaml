apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: ollama
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "5"  # Deploy with AI inference tier
spec:
  project: default
  source:
    repoURL: https://github.com/tzervas/self-hosted-ai.git
    targetRevision: main
    path: helm/ollama
    helm:
      values: |
        # CPU-only Ollama for homelab server (no GPU)
        replicaCount: 1

        image:
          repository: ollama/ollama
          tag: "0.16.2"  # Pin version, never use :latest

        env:
          OLLAMA_HOST: "0.0.0.0"
          OLLAMA_ORIGINS: "*"
          OLLAMA_KEEP_ALIVE: "24h"
          OLLAMA_KV_CACHE_TYPE: "q8_0"
          OLLAMA_NUM_PARALLEL: "2"

        modelsPersistence:
          enabled: true
          size: 50Gi
          storageClass: "longhorn"

        gpu:
          enabled: false
          nvidia:
            enabled: false

        resources:
          requests:
            cpu: 500m
            memory: 2Gi
          limits:
            cpu: 8000m
            memory: 32Gi

        nodeSelector:
          kubernetes.io/hostname: homelab

        networkPolicy:
          enabled: true
          allowedNamespaces:
            - ai-services
            - self-hosted-ai
            - gpu-workloads
            - default
  destination:
    server: https://kubernetes.default.svc
    namespace: ai-services
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
