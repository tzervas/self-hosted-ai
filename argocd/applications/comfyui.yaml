# ComfyUI - Image and Video Generation
# Runs on akula-prime GPU node with local storage
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: comfyui
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "6"  # Deploy after Ollama GPU
spec:
  project: default
  ignoreDifferences:
    - group: ""
      kind: PersistentVolumeClaim
      jsonPointers:
        - /spec
  source:
    repoURL: https://github.com/tzervas/self-hosted-ai.git
    targetRevision: main
    path: helm/gpu-worker
    helm:
      releaseName: comfyui
      values: |
        # Only deploy ComfyUI (disable other components)
        ollama:
          gpu:
            enabled: false

        automatic1111:
          enabled: false

        tts:
          enabled: false

        audio:
          enabled: false

        video:
          enabled: false

        comfyui:
          enabled: true
          image:
            repository: ghcr.io/tzervas/comfyui-blackwell
            tag: "latest"  # Latest build with ComfyUI + Manager + PyTorch 2.7+cu128 + CUDA 12.8
            pullPolicy: IfNotPresent
          persistence:
            enabled: true
            size: 50Gi
            storageClass: longhorn
          service:
            type: ClusterIP
            port: 8188  # Direct ComfyUI (no proxy layer)
          env:
            # VRAM optimization for 16GB GPU (RTX 5080)
            # lowvram: model offloading to RAM when not in use
            # preview-method auto: optimized preview generation
            - name: COMMANDLINE_ARGS
              value: "--lowvram --preview-method auto"
            # PyTorch CUDA memory optimization
            - name: PYTORCH_CUDA_ALLOC_CONF
              value: "max_split_size_mb:512"
          nodeSelector:
            kubernetes.io/hostname: akula-prime
          tolerations:
            - key: nvidia.com/gpu
              operator: Exists
              effect: NoSchedule
          resources:
            requests:
              memory: "4Gi"
              cpu: "1000m"
            limits:
              memory: "16Gi"
              cpu: "4000m"
              nvidia.com/gpu: "1"
  destination:
    server: https://kubernetes.default.svc
    namespace: gpu-workloads
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
      - RespectIgnoreDifferences=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
