# LiteLLM Proxy Configuration
# GPU Inference Queue with Ollama Backend
# Supports priority queues, rate limiting, and load balancing
#
# Deployed Models (verified 2026-02-18):
#   GPU (akula-prime, RTX 5080 16GB):
#     - qwen2.5-coder:14b, phi4, llama3.1:8b, llava:13b, nomic-embed-text
#   CPU (homelab):
#     - mistral:7b, nomic-embed-text

# =============================================================================
# Model Routing Configuration
# =============================================================================

model_list:
  # === GPU Models (akula-prime, RTX 5080) ===

  - model_name: qwen2.5-coder:14b
    litellm_params:
      model: ollama_chat/qwen2.5-coder:14b
      api_base: ${OLLAMA_GPU_URL:-http://ollama-gpu.gpu-workloads:11434}
      timeout: 300
      stream: true
    model_info:
      mode: chat
      supports_function_calling: true
      supports_vision: false

  - model_name: phi4:latest
    litellm_params:
      model: ollama_chat/phi4:latest
      api_base: ${OLLAMA_GPU_URL:-http://ollama-gpu.gpu-workloads:11434}
      timeout: 300
      stream: true
    model_info:
      mode: chat
      supports_function_calling: true

  - model_name: llama3.1:8b
    litellm_params:
      model: ollama_chat/llama3.1:8b
      api_base: ${OLLAMA_GPU_URL:-http://ollama-gpu.gpu-workloads:11434}
      timeout: 300
      stream: true
    model_info:
      mode: chat
      supports_function_calling: true

  # Vision model - supports image analysis
  - model_name: llava:13b
    litellm_params:
      model: ollama_chat/llava:13b
      api_base: ${OLLAMA_GPU_URL:-http://ollama-gpu.gpu-workloads:11434}
      timeout: 300
      stream: true
    model_info:
      mode: chat
      supports_vision: true

  # === CPU Models (homelab, fallback) ===

  - model_name: mistral:7b
    litellm_params:
      model: ollama_chat/mistral:7b
      api_base: ${OLLAMA_CPU_URL:-http://ollama.ai-services:11434}
      timeout: 300
      stream: true
    model_info:
      mode: chat

  # === Embedding Models ===

  # Primary embedding model on CPU (doesn't compete with GPU inference)
  - model_name: nomic-embed-text
    litellm_params:
      model: ollama/nomic-embed-text
      api_base: ${OLLAMA_CPU_URL:-http://ollama.ai-services:11434}
      timeout: 60
    model_info:
      mode: embedding

  # GPU embedding (faster, for batch jobs)
  - model_name: nomic-embed-text-gpu
    litellm_params:
      model: ollama/nomic-embed-text
      api_base: ${OLLAMA_GPU_URL:-http://ollama-gpu.gpu-workloads:11434}
      timeout: 60
    model_info:
      mode: embedding

# =============================================================================
# Router Settings (Load Balancing & Fallbacks)
# =============================================================================

router_settings:
  routing_strategy: least-busy

  # Fallback chains (GPU â†’ CPU)
  enable_fallbacks: true
  fallbacks:
    - qwen2.5-coder:14b: [mistral:7b]
    - phi4:latest: [llama3.1:8b, mistral:7b]
    - llama3.1:8b: [mistral:7b]
    - llava:13b: [phi4:latest]

  # Retry on transient failures
  num_retries: 3
  retry_after: 5
  timeout: 300

  # Cooldown on sustained failures
  allowed_fails: 3
  cooldown_time: 60

  # Redis-backed response caching
  cache: true
  cache_params:
    type: redis
    host: ${REDIS_HOST:-redis-master.ai-services}
    port: ${REDIS_PORT:-6379}
    ttl: 3600

# =============================================================================
# General Settings
# =============================================================================

general_settings:
  master_key: ${LITELLM_MASTER_KEY}
  store_model_in_db: false
  disable_spend_logs: true
  database_connection_pool_limit: 20
  database_connection_timeout: 30
  alerting_threshold: 300
  json_logs: true
  detailed_debug: false
  health_check_interval: 30

# =============================================================================
# LiteLLM Settings (Priority Queue & Callbacks)
# =============================================================================

litellm_settings:
  enable_request_priority: true

  priority_queue:
    enabled: true
    redis_host: ${REDIS_HOST:-redis-master.ai-services}
    redis_port: ${REDIS_PORT:-6379}

    levels:
      high: 1      # Agent server, critical workflows
      normal: 5    # Regular user requests
      low: 10      # Batch processing, background tasks

    max_queue_size:
      high: 50
      normal: 200
      low: 500

    queue_timeout:
      high: 30
      normal: 120
      low: 600

  # Prometheus metrics
  success_callback:
    - prometheus
  failure_callback:
    - prometheus

  # Response caching via Redis
  cache: true
  cache_params:
    type: redis
    host: ${REDIS_HOST:-redis-master.ai-services}
    port: ${REDIS_PORT:-6379}
    namespace: litellm_cache
    ttl: 3600

  # Guardrails - prevent runaway requests
  max_budget: 100  # Monthly budget cap (in USD equivalent)
  budget_duration: 30d

  # Request limits per model
  model_rpm:
    qwen2.5-coder:14b: 30
    phi4:latest: 30
    llama3.1:8b: 60
    llava:13b: 20
    mistral:7b: 60
    nomic-embed-text: 120

  # Token limits per request
  max_tokens: 32000

# =============================================================================
# Prometheus Metrics
# =============================================================================

prometheus:
  enabled: true
  port: 9090
  path: /metrics

  custom_metrics:
    - name: litellm_request_queue_size
      description: Current size of the request queue by priority
      type: gauge
      labels:
        - priority
    - name: litellm_request_latency_seconds
      description: Request latency in seconds
      type: histogram
      labels:
        - model
        - status
    - name: litellm_token_usage_total
      description: Total tokens used
      type: counter
      labels:
        - model
        - type
