# Model Manifest for Self-Hosted AI Stack
# Tracks deployed models across GPU and CPU Ollama instances
# Used by bootstrap.sh to ensure required models are available
#
# Last verified: 2026-02-18
#
# RTX 5080 VRAM Budget: 16GB
#   - Keep loaded models under 14GB for comfortable inference
#   - Models swap in/out of VRAM automatically (OLLAMA_KEEP_ALIVE: 24h)

# GPU Worker Models (akula-prime, RTX 5080 16GB)
gpu_worker:
  host: ${GPU_WORKER_HOST:-ollama-gpu.gpu-workloads}
  port: ${GPU_WORKER_PORT:-11434}
  models:
    # === CODE GENERATION ===
    - name: qwen2.5-coder:14b
      size: 9.0GB
      purpose: coding
      priority: required
      status: deployed
      capabilities: [code_generation, code_review, refactoring, debugging, function_calling]

    # === REASONING & PROBLEM SOLVING ===
    - name: phi4:latest
      size: 9.1GB
      purpose: reasoning
      priority: required
      status: deployed
      capabilities: [logical_reasoning, mathematics, analysis, function_calling]

    # Fast general-purpose model
    - name: llama3.1:8b
      size: 4.9GB
      purpose: general
      priority: required
      status: deployed
      capabilities: [instruction_following, reasoning, chat, function_calling]

    # === VISION & MULTI-MODAL ===
    - name: llava:13b
      size: 8.0GB
      purpose: vision
      priority: required
      status: deployed
      capabilities: [image_understanding, ocr, visual_reasoning, image_description]

    # === EMBEDDINGS ===
    - name: nomic-embed-text:latest
      size: 274MB
      purpose: embeddings
      priority: required
      status: deployed
      capabilities: [text_embeddings, semantic_search, rag]

# CPU Server Models (homelab)
# Fallback models when GPU is busy, plus dedicated embedding
cpu_server:
  host: ${CPU_SERVER_HOST:-ollama.ai-services}
  port: ${CPU_SERVER_PORT:-11434}
  models:
    # General chat fallback
    - name: mistral:7b
      size: 4.4GB
      purpose: chat
      priority: required
      status: deployed
      capabilities: [chat, general_qa, summarization]

    # Primary embedding model (runs on CPU to avoid GPU contention)
    - name: nomic-embed-text:latest
      size: 274MB
      purpose: embeddings
      priority: required
      status: deployed
      capabilities: [text_embeddings, semantic_search, rag, document_retrieval]

# =============================================================================
# Future Models (not yet deployed)
# =============================================================================
# These models are planned but not currently pulled. Add them to the
# appropriate section above and run `ollama pull` when ready.
#
# GPU candidates:
#   - deepseek-coder-v2:16b (coding, 8GB) - excellent for code generation
#   - codellama:13b (coding, 7GB) - fast code completion
#   - mistral:7b-instruct-v0.3 (function_calling, 4.1GB) - tool use specialist
#
# CPU candidates:
#   - phi3:latest (chat, 2GB) - lightweight fast model
#   - gemma2:2b (chat, 1.6GB) - smallest model for testing
#   - mxbai-embed-large:latest (embeddings, 669MB) - higher quality embeddings
#   - sqlcoder:15b (sql, 8.5GB) - SQL generation specialist
#
# HuggingFace models (require HF_TOKEN):
#   - See archive/models-manifest-full.yml for the complete list of
#     uncensored, TTS, audio, and video models planned for future phases

# =============================================================================
# ComfyUI Models (Image/Video Generation)
# =============================================================================
# Downloaded directly to ComfyUI model directories on GPU worker
# ComfyUI is currently scaled to 0 (GPU shared with Ollama)

comfyui:
  base_path: /data/comfyui/models
  status: disabled  # Scaled to 0, enable when GPU time-slicing is configured
  models:
    - type: checkpoint
      name: sd_xl_base_1.0.safetensors
      url: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors
      path: checkpoints/
      size_gb: 6.5
      purpose: image_generation
      priority: required

    - type: upscale_models
      name: RealESRGAN_x4plus.pth
      url: https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth
      path: upscale_models/
      size_gb: 0.064
      purpose: upscaling
      priority: required

    - type: vae
      name: sdxl_vae.safetensors
      url: https://huggingface.co/stabilityai/sdxl-vae/resolve/main/sdxl_vae.safetensors
      path: vae/
      size_gb: 0.335
      purpose: vae
      priority: required
