# Model Manifest for Self-Hosted AI Stack
# Used by bootstrap.sh to ensure required models are available
# Models are checked against Ollama API before pulling (no re-downloads)

# GPU Worker Models (akula-prime)
# Optimized for RTX 5080 16GB VRAM - coding focus
gpu_worker:
  host: ${GPU_WORKER_HOST:-192.168.1.99}
  port: ${GPU_WORKER_PORT:-11434}
  models:
    # Primary coding model - excellent for Rust, Python, Shell
    - name: qwen2.5-coder:14b
      size: 8GB
      purpose: coding
      priority: required

    # Reasoning + coding - good for complex problems
    - name: phi4:latest
      size: 8GB
      purpose: reasoning
      priority: required

    # Advanced coding - alternative for variety
    - name: deepseek-coder-v2:16b
      size: 8GB
      purpose: coding
      priority: optional

# CPU Server Models (homelab)
# Optimized for 120GB RAM - fallback and embeddings
cpu_server:
  host: ${CPU_SERVER_HOST:-192.168.1.170}
  port: ${CPU_SERVER_PORT:-11434}
  models:
    # General chat fallback
    - name: mistral:7b
      size: 4GB
      purpose: chat
      priority: required

    # Fast lightweight model
    - name: phi3:latest
      size: 2GB
      purpose: chat
      priority: optional

    # Embeddings for RAG
    - name: nomic-embed-text:latest
      size: 274MB
      purpose: embeddings
      priority: required
