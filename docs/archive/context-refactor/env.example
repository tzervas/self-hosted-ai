# =============================================================================
# Self-Hosted AI Development Platform - Environment Configuration
# =============================================================================
# Version: 2.0.0
# Last Updated: 2026-01-12
# Target: k3s cluster with RTX 5080 GPU (16GB VRAM)
# =============================================================================

# -----------------------------------------------------------------------------
# CLUSTER CONFIGURATION
# -----------------------------------------------------------------------------
CLUSTER_NAME=homelab-ai
CLUSTER_DOMAIN=homelab.local
KUBECONFIG=/etc/rancher/k3s/k3s.yaml

# K3s specific
K3S_TOKEN=                               # Generated during k3s install
K3S_NODE_NAME=gpu-node-01
K3S_EXTERNAL_IP=                         # Your LAN IP

# -----------------------------------------------------------------------------
# ARGOCD CONFIGURATION
# -----------------------------------------------------------------------------
ARGOCD_NAMESPACE=argocd
ARGOCD_EXTERNAL_URL=https://argocd.${CLUSTER_DOMAIN}
ARGOCD_ADMIN_PASSWORD=                   # Auto-generated if empty, retrieve via: kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d

# GitOps Repository
GITOPS_REPO_URL=https://github.com/your-org/gitops-platform.git
GITOPS_REPO_BRANCH=main
GITOPS_REPO_PATH=argocd/applications

# -----------------------------------------------------------------------------
# STORAGE CONFIGURATION
# -----------------------------------------------------------------------------
DEFAULT_STORAGE_CLASS=longhorn           # longhorn | local-path
LONGHORN_REPLICAS=2                      # Redundancy level (1 for single-node, 2+ for multi-node)
LONGHORN_DATA_PATH=/var/lib/longhorn     # Host path for storage

# Storage Sizes (adjust based on available disk)
GITLAB_GITALY_SIZE=50Gi
GITLAB_MINIO_SIZE=20Gi
GITLAB_POSTGRES_SIZE=8Gi
GITLAB_REDIS_SIZE=5Gi
OLLAMA_MODELS_SIZE=200Gi
DIFY_POSTGRES_SIZE=20Gi
DIFY_WEAVIATE_SIZE=20Gi
DIFY_STORAGE_SIZE=50Gi

# -----------------------------------------------------------------------------
# GPU CONFIGURATION
# -----------------------------------------------------------------------------
GPU_OPERATOR_NAMESPACE=gpu-operator
GPU_TIME_SLICE_REPLICAS=4                # Virtual GPU replicas from single RTX 5080
GPU_DRIVER_ENABLED=false                 # false = use pre-installed host drivers
GPU_TOOLKIT_CONTAINERD_SOCKET=/run/k3s/containerd/containerd.sock
GPU_TOOLKIT_CONTAINERD_CONFIG=/var/lib/rancher/k3s/agent/etc/containerd/config.toml

# NVIDIA Driver (if GPU_DRIVER_ENABLED=true)
NVIDIA_DRIVER_VERSION=570               # Required for RTX 5080 Blackwell
CUDA_VERSION=12.8                       # Minimum for Blackwell architecture

# -----------------------------------------------------------------------------
# GITLAB CONFIGURATION
# -----------------------------------------------------------------------------
GITLAB_NAMESPACE=gitlab
GITLAB_EXTERNAL_URL=https://gitlab.${CLUSTER_DOMAIN}
GITLAB_REGISTRY_URL=https://registry.gitlab.${CLUSTER_DOMAIN}
GITLAB_EDITION=ce                        # ce | ee

# GitLab Root Account
GITLAB_ROOT_EMAIL=admin@${CLUSTER_DOMAIN}
GITLAB_ROOT_PASSWORD=                    # Min 8 characters, special chars recommended

# GitLab Database
GITLAB_POSTGRES_USER=gitlab
GITLAB_POSTGRES_PASSWORD=                # Auto-generated if empty
GITLAB_POSTGRES_DATABASE=gitlabhq_production

# GitLab Redis
GITLAB_REDIS_PASSWORD=                   # Auto-generated if empty

# GitLab MinIO (Object Storage)
GITLAB_MINIO_ACCESS_KEY=                 # Auto-generated if empty
GITLAB_MINIO_SECRET_KEY=                 # Auto-generated if empty

# GitLab Ingress
GITLAB_INGRESS_CLASS=traefik             # traefik | nginx
GITLAB_TLS_ENABLED=true
GITLAB_TLS_SECRET_NAME=gitlab-tls

# Resource Limits (tuned for 16GB RAM node)
GITLAB_WEBSERVICE_MEMORY_LIMIT=3Gi
GITLAB_SIDEKIQ_MEMORY_LIMIT=2Gi
GITLAB_GITALY_MEMORY_LIMIT=1Gi

# Optional Components (disable to save resources)
GITLAB_PAGES_ENABLED=false
GITLAB_KAS_ENABLED=false                 # Kubernetes Agent Server
GITLAB_MATTERMOST_ENABLED=false

# -----------------------------------------------------------------------------
# GITLAB RUNNER CONFIGURATION
# -----------------------------------------------------------------------------
GITLAB_RUNNER_NAMESPACE=gitlab-runner

# Runner Registration (GitLab 16+ uses authentication tokens)
# Create via: GitLab Admin → CI/CD → Runners → New Instance Runner
GITLAB_RUNNER_TOKEN=                     # glrt-xxx format

# Runner Configuration
GITLAB_RUNNER_CONCURRENT=10              # Max concurrent jobs
GITLAB_RUNNER_CHECK_INTERVAL=30          # Seconds between job checks
GITLAB_RUNNER_DEFAULT_IMAGE=ubuntu:22.04
GITLAB_RUNNER_TAGS=kubernetes,docker,amd64,arm64,riscv64
GITLAB_RUNNER_PRIVILEGED=true            # Required for Docker-in-Docker

# Build Resources
GITLAB_RUNNER_CPU_REQUEST=500m
GITLAB_RUNNER_CPU_LIMIT=2
GITLAB_RUNNER_MEMORY_REQUEST=1Gi
GITLAB_RUNNER_MEMORY_LIMIT=4Gi

# GPU Jobs (optional - shares time-sliced GPU with Ollama)
GITLAB_RUNNER_GPU_ENABLED=false          # Enable GPU access in CI jobs
GITLAB_RUNNER_GPU_NODE_SELECTOR=nvidia.com/gpu=present

# Build Cache (S3/MinIO)
GITLAB_RUNNER_CACHE_TYPE=s3
GITLAB_RUNNER_CACHE_BUCKET=gitlab-runner-cache
GITLAB_RUNNER_CACHE_S3_SERVER=gitlab-minio.gitlab.svc.cluster.local:9000
GITLAB_RUNNER_CACHE_S3_ACCESS_KEY=${GITLAB_MINIO_ACCESS_KEY}
GITLAB_RUNNER_CACHE_S3_SECRET_KEY=${GITLAB_MINIO_SECRET_KEY}

# -----------------------------------------------------------------------------
# GITHUB MIRRORING (for release pushes)
# -----------------------------------------------------------------------------
GITHUB_MIRROR_ENABLED=true
GITHUB_ORG=your-github-org
GITHUB_TOKEN=                            # PAT with repo scope
GITHUB_MIRROR_BRANCHES=main              # Comma-separated branches to mirror
GITHUB_MIRROR_TAGS=true                  # Push tags (releases)

# -----------------------------------------------------------------------------
# OLLAMA CONFIGURATION
# -----------------------------------------------------------------------------
OLLAMA_NAMESPACE=ollama
OLLAMA_EXTERNAL_URL=https://ollama.${CLUSTER_DOMAIN}
OLLAMA_API_URL=http://ollama.ollama.svc.cluster.local:11434

# Model Configuration (verified to fit in 16GB VRAM)
OLLAMA_PRIMARY_MODEL=llama3.2:8b         # ~5GB VRAM
OLLAMA_CODING_MODEL=codellama:13b        # ~8GB VRAM
OLLAMA_FAST_MODEL=qwen2.5-coder:7b-instruct  # ~4GB VRAM
OLLAMA_EMBED_MODEL=nomic-embed-text      # ~274MB

# Custom Modelfiles
OLLAMA_CUSTOM_CODING_ASSISTANT=coding-assistant
OLLAMA_CUSTOM_DEVOPS_SPECIALIST=devops-specialist

# Performance Tuning
OLLAMA_NUM_PARALLEL=2                    # Concurrent requests
OLLAMA_MAX_LOADED_MODELS=2               # Models kept in VRAM
OLLAMA_KEEP_ALIVE=24h                    # Keep model loaded (reduces swap latency)
OLLAMA_NUM_CTX=32768                     # Context window (primary model)

# Resources
OLLAMA_CPU_REQUEST=2
OLLAMA_CPU_LIMIT=4
OLLAMA_MEMORY_REQUEST=8Gi
OLLAMA_MEMORY_LIMIT=16Gi
OLLAMA_GPU_REQUEST=1                     # Time-sliced replica

# -----------------------------------------------------------------------------
# DIFY CONFIGURATION
# -----------------------------------------------------------------------------
DIFY_NAMESPACE=dify
DIFY_EXTERNAL_URL=https://dify.${CLUSTER_DOMAIN}
DIFY_VERSION=1.10.1

# Dify Database
DIFY_POSTGRES_USER=dify
DIFY_POSTGRES_PASSWORD=
DIFY_POSTGRES_DATABASE=dify

# Dify Redis
DIFY_REDIS_PASSWORD=

# Dify LLM Backend
DIFY_OLLAMA_BASE_URL=${OLLAMA_API_URL}
DIFY_DEFAULT_MODEL=${OLLAMA_PRIMARY_MODEL}

# Dify RAG Configuration
DIFY_VECTOR_STORE=weaviate               # weaviate | qdrant | milvus | pgvector
DIFY_EMBEDDING_MODEL=${OLLAMA_EMBED_MODEL}
DIFY_RETRIEVAL_MODE=hybrid               # semantic | keyword | hybrid
DIFY_RERANK_ENABLED=true

# Dify Resources
DIFY_API_REPLICAS=1
DIFY_WORKER_REPLICAS=1
DIFY_WEB_REPLICAS=1

# -----------------------------------------------------------------------------
# N8N CONFIGURATION (Workflow Automation)
# -----------------------------------------------------------------------------
N8N_NAMESPACE=n8n
N8N_EXTERNAL_URL=https://n8n.${CLUSTER_DOMAIN}
N8N_BASIC_AUTH_USER=admin
N8N_BASIC_AUTH_PASSWORD=
N8N_ENCRYPTION_KEY=                      # Auto-generated if empty

# N8N Integrations
N8N_OLLAMA_URL=${OLLAMA_API_URL}
N8N_GITLAB_URL=${GITLAB_EXTERNAL_URL}
N8N_DIFY_URL=${DIFY_EXTERNAL_URL}

# -----------------------------------------------------------------------------
# CONTINUE.DEV CONFIGURATION (VS Code Extension)
# -----------------------------------------------------------------------------
# Note: Continue.dev uses config.yaml (NOT config.json - deprecated)
# Location: ~/.continue/config.yaml
# Environment variables map to Continue.dev YAML config keys:
# - CONTINUE_OLLAMA_API_BASE → apiBase
# - CONTINUE_CHAT_MODEL → models.chat
# - CONTINUE_AUTOCOMPLETE_MODEL → models.autocomplete
# - CONTINUE_EMBED_MODEL → models.embeddings

CONTINUE_OLLAMA_API_BASE=${OLLAMA_API_URL}
CONTINUE_CHAT_MODEL=${OLLAMA_CUSTOM_CODING_ASSISTANT}
CONTINUE_AUTOCOMPLETE_MODEL=${OLLAMA_FAST_MODEL}
CONTINUE_EMBED_MODEL=${OLLAMA_EMBED_MODEL}

# -----------------------------------------------------------------------------
# TLS/CERTIFICATE CONFIGURATION
# -----------------------------------------------------------------------------
TLS_CERT_SOURCE=selfsigned               # letsencrypt | selfsigned | custom
LETSENCRYPT_EMAIL=admin@${CLUSTER_DOMAIN}
LETSENCRYPT_SERVER=https://acme-v02.api.letsencrypt.org/directory  # Production
# LETSENCRYPT_SERVER=https://acme-staging-v02.api.letsencrypt.org/directory  # Staging

# For custom certificates
TLS_CUSTOM_CERT_PATH=
TLS_CUSTOM_KEY_PATH=
TLS_CUSTOM_CA_PATH=

# -----------------------------------------------------------------------------
# SECURITY CONFIGURATION
# -----------------------------------------------------------------------------
# Sealed Secrets (for encrypting secrets in Git)
SEALED_SECRETS_NAMESPACE=kube-system
SEALED_SECRETS_CONTROLLER_NAME=sealed-secrets-controller

# Pod Security
POD_SECURITY_LEVEL=baseline              # privileged | baseline | restricted

# Network Policies
NETWORK_POLICIES_ENABLED=true
DEFAULT_DENY_INGRESS=true
DEFAULT_DENY_EGRESS=false                # true can break external DNS

# -----------------------------------------------------------------------------
# OBSERVABILITY (Optional)
# -----------------------------------------------------------------------------
PROMETHEUS_ENABLED=false                  # Enable for metrics (adds resource overhead)
GRAFANA_ENABLED=false
LOKI_ENABLED=false                        # Centralized logging

# If enabled
PROMETHEUS_RETENTION=7d
GRAFANA_ADMIN_PASSWORD=

# -----------------------------------------------------------------------------
# SPEC-DRIVEN DEVELOPMENT CONFIGURATION
# -----------------------------------------------------------------------------
# Spec validation schema version
SPEC_SCHEMA_VERSION=v1

# LLM Code Review
LLM_REVIEW_MODEL=${OLLAMA_CUSTOM_CODING_ASSISTANT}
LLM_REVIEW_MIN_SCORE=8                    # Minimum score (1-10) to pass review
LLM_REVIEW_MAX_DIFF_SIZE=10000            # Chars of diff to send to LLM

# Spec Validation
SPEC_VALIDATION_ENABLED=true
SPEC_REQUIRED_SECTIONS=requirements,success_criteria,deliverables,attribution

# -----------------------------------------------------------------------------
# MULTI-ARCHITECTURE BUILD CONFIGURATION
# -----------------------------------------------------------------------------
# Target architectures
BUILD_ARCH_AMD64=true
BUILD_ARCH_ARM64=true
BUILD_ARCH_RISCV64=true
BUILD_ARCH_HOLOGRAPHIC=false              # Your custom architecture

# QEMU for cross-architecture emulation
QEMU_PLATFORMS=linux/amd64,linux/arm64,linux/riscv64

# Cross-compilation toolchains
RUST_TARGETS=x86_64-unknown-linux-gnu,aarch64-unknown-linux-gnu,riscv64gc-unknown-linux-gnu
GO_TARGETS=linux/amd64,linux/arm64,linux/riscv64

# -----------------------------------------------------------------------------
# BACKUP CONFIGURATION
# -----------------------------------------------------------------------------
BACKUP_ENABLED=false
BACKUP_SCHEDULE="0 2 * * *"              # Daily at 2 AM
BACKUP_RETENTION_DAYS=7
BACKUP_S3_BUCKET=
BACKUP_S3_ENDPOINT=
BACKUP_S3_ACCESS_KEY=
BACKUP_S3_SECRET_KEY=

# -----------------------------------------------------------------------------
# DEVELOPMENT OVERRIDES
# -----------------------------------------------------------------------------
# Use these in development/testing environments

# Reduce resource requests for testing
DEV_MODE=false
DEV_RESOURCE_MULTIPLIER=0.5              # Multiply all resource requests by this

# Skip certain validations
DEV_SKIP_TLS_VERIFY=false
DEV_SKIP_NETWORK_POLICIES=false

# Enable debug logging
DEBUG_OLLAMA=false
DEBUG_DIFY=false
DEBUG_GITLAB=false
DEBUG_ARGOCD=false

# =============================================================================
# NOTES
# =============================================================================
#
# 1. VRAM CONSTRAINTS: RTX 5080 has 16GB VRAM. Maximum recommended model is
#    ~20B parameters at Q4 quantization. 72B models WILL NOT FIT.
#
# 2. TIME-SLICING: GPU is shared between Ollama and CI jobs via 4 virtual
#    replicas. This allows concurrent usage but reduces per-workload performance.
#
# 3. STORAGE: Ensure at least 400GB available for full stack:
#    - Ollama models: ~200GB
#    - GitLab data: ~100GB
#    - Dify knowledge base: ~50GB
#    - Buffer: ~50GB
#
# 4. MEMORY: Full stack requires ~12-14GB RAM. 16GB node is tight but viable.
#
# 5. SECRETS: Never commit this file with real values to Git.
#    Use `sealed-secrets` to encrypt secrets before committing.
#
# 6. RUNNER TOKENS: GitLab 16+ uses authentication tokens (glrt-xxx).
#    Registration tokens are DEPRECATED and will not work.
#
# =============================================================================
