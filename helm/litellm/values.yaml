# Default values for LiteLLM proxy
# GPU inference queue with Ollama backend

replicaCount: 1

image:
  repository: ghcr.io/berriai/litellm
  tag: v1.80.11-stable
  pullPolicy: IfNotPresent

nameOverride: ""
fullnameOverride: "litellm"

# Database configuration - disabled by default due to Prisma issues
database:
  enabled: false

service:
  type: ClusterIP
  port: 4000
  metricsPort: 9090

ingress:
  enabled: true
  className: traefik
  annotations:
    traefik.ingress.kubernetes.io/router.entrypoints: websecure
    traefik.ingress.kubernetes.io/router.tls: "true"
    cert-manager.io/cluster-issuer: letsencrypt-production
  hosts:
    - host: llm.vectorweight.com
      paths:
        - path: /
          pathType: Prefix
  tls:
    - hosts:
        - llm.vectorweight.com
      secretName: vectorweight-wildcard-tls

# Environment variables
env:
  OLLAMA_GPU_URL: "http://ollama-gpu.gpu-workloads:11434"
  OLLAMA_CPU_URL: "http://ollama.ai-services:11434"
  REDIS_HOST: "redis-master.ai-services"
  REDIS_PORT: "6379"
  # Prisma cache directory (must be writable)
  PRISMA_HOME: "/tmp/prisma"
  HOME: "/tmp"
  # Disable store_model_in_db to avoid Prisma query engine issues
  STORE_MODEL_IN_DB: "false"
  DISABLE_SPEND_LOGS: "true"

# Secret references
secrets:
  litellmMasterKey:
    secretName: litellm-secret
    key: master-key
  databaseUrl:
    secretName: litellm-secret
    key: database-url

# Config file mount
config:
  enabled: true
  configMapName: litellm-config
  mountPath: /app/config.yaml
  subPath: config.yaml

# Resource allocation - sufficient for handling concurrent requests
resources:
  requests:
    cpu: 500m
    memory: 1Gi
  limits:
    cpu: 2000m
    memory: 4Gi

# Health checks - startup probe handles slow initialization
startupProbe:
  httpGet:
    path: /health/liveliness
    port: 4000
  initialDelaySeconds: 5
  periodSeconds: 5
  timeoutSeconds: 5
  failureThreshold: 30  # 30 * 5s = 2.5 min max startup
  successThreshold: 1

livenessProbe:
  httpGet:
    path: /health/liveliness
    port: 4000
  initialDelaySeconds: 10  # Reduced - startup probe handles slow starts
  periodSeconds: 30
  timeoutSeconds: 10
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /health/readiness
    port: 4000
  initialDelaySeconds: 5  # Reduced - startup probe handles slow starts
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

# Pod configuration
nodeSelector: {}

tolerations: []

affinity: {}

# Horizontal Pod Autoscaler - scale based on CPU/memory
autoscaling:
  enabled: true
  minReplicas: 1
  maxReplicas: 3
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80
  scaleDownStabilizationWindowSeconds: 300
  scaleUpStabilizationWindowSeconds: 0

# Vertical Pod Autoscaler - recommend optimal resources
# NOTE: Requires VPA CRD to be installed (not installed in this cluster)
vpa:
  enabled: false
  updateMode: "Off"  # "Off" = recommendations only, "Auto" = automatic updates
  minAllowed:
    cpu: 100m
    memory: 256Mi
  maxAllowed:
    cpu: 4
    memory: 8Gi

# Service account
serviceAccount:
  create: true
  annotations: {}
  name: ""

# Pod security context
podSecurityContext:
  fsGroup: 1000

securityContext:
  runAsNonRoot: true
  runAsUser: 1000
  capabilities:
    drop:
      - ALL
  readOnlyRootFilesystem: false
