# Default values for self-hosted-ai-server
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# Global configuration
global:
  imageRegistry: ""
  imagePullSecrets: []
  storageClass: ""

# Open WebUI configuration
openwebui:
  enabled: true
  image:
    repository: ghcr.io/open-webui/open-webui
    tag: v0.8.5
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 8080
    annotations: {}
  ingress:
    enabled: false
    className: ""
    annotations: {}
    hosts:
      - host: openwebui.local
        paths:
          - path: /
            pathType: Prefix
    tls: []
  env:
    - name: OLLAMA_BASE_URLS
      value: "http://ollama-cpu:11434;http://gpu-worker-ollama:11434"
    - name: WEBUI_SECRET_KEY
      valueFrom:
        secretKeyRef:
          name: webui-secret
          key: secret-key
    - name: WEBUI_NAME
      value: "Self-Hosted AI Server"
    - name: ENABLE_SIGNUP
      value: "true"
    - name: ENABLE_COMMUNITY_SHARING
      value: "false"
    - name: ENABLE_RAG_WEB_SEARCH
      value: "false"
    - name: RAG_WEB_SEARCH_ENGINE
      value: "searxng"
    - name: ENABLE_IMAGE_GENERATION
      value: "true"
    - name: IMAGE_GENERATION_ENGINE
      value: "comfyui"
    - name: COMFYUI_BASE_URL
      value: "http://gpu-worker-comfyui:8188"
    - name: ENABLE_CODE_EXECUTION
      value: "true"
    - name: CODE_EXECUTION_ENGINE
      value: "gvisor"
  persistence:
    enabled: true
    size: 10Gi
    storageClass: ""
    accessModes:
      - ReadWriteOnce
    annotations: {}

# Ollama CPU configuration
ollama:
  cpu:
    enabled: true
    image:
      repository: ollama/ollama
      tag: 0.15.2
      pullPolicy: IfNotPresent
    service:
      type: ClusterIP
      port: 11434
      annotations: {}
    env:
      - name: OLLAMA_NUM_PARALLEL
        value: "8"
      - name: OLLAMA_MAX_LOADED_MODELS
        value: "4"
      - name: OLLAMA_KEEP_ALIVE
        value: "30m"
      - name: OLLAMA_NUM_THREADS
        value: "56"
    persistence:
      enabled: true
      size: 50Gi
      storageClass: ""
      accessModes:
        - ReadWriteOnce
      annotations: {}

# Redis configuration
redis:
  enabled: true
  image:
    repository: redis
    tag: 8.5.2-alpine
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 6379
    annotations: {}
  persistence:
    enabled: true
    size: 5Gi
    storageClass: ""
    accessModes:
      - ReadWriteOnce
    annotations: {}
  config:
    appendonly: "yes"

# Monitoring configuration
monitoring:
  enabled: true
  prometheus:
    enabled: true
    image:
      repository: prom/prometheus
      tag: v2.56.1
      pullPolicy: IfNotPresent
    service:
      type: ClusterIP
      port: 9090
      annotations: {}
    persistence:
      enabled: true
      size: 10Gi
      storageClass: ""
      accessModes:
        - ReadWriteOnce
      annotations: {}
    config:
      global:
        scrape_interval: 15s
      rule_files:
        - /etc/prometheus/alert_rules.yml
      alerting:
        alertmanagers:
          - static_configs:
              - targets:
                - alertmanager:9093
      scrape_configs:
        - job_name: 'prometheus'
          static_configs:
            - targets: ['localhost:9090']
        - job_name: 'openwebui'
          static_configs:
            - targets: ['openwebui:8080']
        - job_name: 'ollama'
          static_configs:
            - targets: ['ollama-cpu:11434']
        - job_name: 'redis'
          static_configs:
            - targets: ['redis:6379']

  grafana:
    enabled: true
    image:
      repository: grafana/grafana
      tag: 11.5.0
      pullPolicy: IfNotPresent
    service:
      type: ClusterIP
      port: 3000
      annotations: {}
    adminUser: admin
    adminPassword: admin
    persistence:
      enabled: true
      size: 5Gi
      storageClass: ""
      accessModes:
        - ReadWriteOnce
      annotations: {}

  loki:
    enabled: true
    image:
      repository: grafana/loki
      tag: 3.3.2
      pullPolicy: IfNotPresent
    service:
      type: ClusterIP
      port: 3100
      annotations: {}
    persistence:
      enabled: true
      size: 10Gi
      storageClass: ""
      accessModes:
        - ReadWriteOnce
      annotations: {}

  promtail:
    enabled: true
    image:
      repository: grafana/promtail
      tag: 3.3.2
      pullPolicy: IfNotPresent
    config:
      server:
        http_listen_port: 9080
        grpc_listen_port: 0
      positions:
        filename: /tmp/positions.yaml
      clients:
        - url: http://loki:3100/loki/api/v1/push
      scrape_configs:
        - job_name: kubernetes-pods
          kubernetes_sd_configs:
            - role: pod
          pipeline_stages:
            - docker: {}
          relabel_configs:
            - source_labels:
                - __meta_kubernetes_pod_node_name
              target_label: __host__
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - action: replace
              replacement: $1
              separator: /
              source_labels:
                - __meta_kubernetes_namespace
                - __meta_kubernetes_pod_name
              target_label: job
            - action: replace
              source_labels:
                - __meta_kubernetes_namespace
              target_label: namespace
            - action: replace
              source_labels:
                - __meta_kubernetes_pod_name
              target_label: pod
            - action: replace
              source_labels:
                - __meta_kubernetes_pod_name
              target_label: container
            - action: replace
              replacement: /var/log/pods/*$1/*.log
              separator: /
              source_labels:
                - __meta_kubernetes_pod_uid
                - __meta_kubernetes_pod_container_name
              target_label: __path__

  nodeExporter:
    enabled: true
    image:
      repository: prom/node-exporter
      tag: v1.9.0
      pullPolicy: IfNotPresent
    service:
      type: ClusterIP
      port: 9100
      annotations: {}

# LiteLLM Proxy configuration (GPU inference queue)
litellm:
  enabled: true
  image:
    repository: ghcr.io/berriai/litellm
    tag: v1.52.4
    pullPolicy: IfNotPresent
  replicas: 1
  service:
    type: ClusterIP
    port: 4000
  secretName: litellm-secrets
  ollamaGpuUrl: "http://gpu-worker-ollama:11434"
  ollamaCpuUrl: "http://ollama-cpu:11434"
  maxParallelRequests: 100
  requestTimeout: 600
  debug: false
  redis:
    host: redis
    port: 6379
  router:
    strategy: least-busy
    enableFallbacks: true
    fallbacks:
      - qwen2.5-coder:14b: [codellama:13b, mistral:7b]
      - llama3.1:70b-instruct-q4_0: [llama3.1:8b, phi4:latest]
    numRetries: 3
    retryAfter: 5
    timeout: 300
    allowedFails: 3
    cooldownTime: 60
    cache: true
    cacheTtl: 3600
  queue:
    maxSize:
      high: 50
      normal: 200
      low: 500
    timeout:
      high: 30
      normal: 120
      low: 600
  guardrails:
    maxInputTokens: 128000
    maxOutputTokens: 32000
  langfuse:
    enabled: false
    host: "https://cloud.langfuse.com"
  models:
    - name: qwen2.5-coder:14b
      litellm_model: ollama/qwen2.5-coder:14b
      api_base: "${OLLAMA_GPU_URL}"
      timeout: 300
      model_info:
        mode: chat
        supports_function_calling: true
    - name: llama3.1:8b
      litellm_model: ollama/llama3.1:8b
      api_base: "${OLLAMA_GPU_URL}"
      timeout: 300
      model_info:
        mode: chat
        supports_function_calling: true
    - name: llava:13b
      litellm_model: ollama/llava:13b
      api_base: "${OLLAMA_GPU_URL}"
      timeout: 300
      model_info:
        mode: chat
        supports_vision: true
    - name: nomic-embed-text
      litellm_model: ollama/nomic-embed-text
      api_base: "${OLLAMA_CPU_URL}"
      timeout: 60
      model_info:
        mode: embedding
  resources:
    requests:
      memory: "512Mi"
      cpu: "250m"
    limits:
      memory: "2Gi"
      cpu: "1000m"

# Backup configuration
backup:
  # Velero cluster-wide backups
  velero:
    enabled: false
    schedules:
      daily: "0 2 * * *"
      weekly: "0 3 * * 0"
    includedNamespaces:
      - self-hosted-ai
    snapshotVolumes: true
    defaultVolumesToFsBackup: true
    storageLocation: default
    retention:
      daily: "168h"   # 7 days
      weekly: "672h"  # 4 weeks
    backupStorageLocation:
      create: false
      provider: aws
      bucket: ""
      prefix: self-hosted-ai
      config: {}
      credentialSecret: velero-credentials
  
  # Component-specific backups
  components:
    enabled: true
    storage:
      size: 50Gi
      storageClass: ""
    postgres:
      schedule: "0 * * * *"  # Hourly
      host: postgres
      port: "5432"
      database: litellm
      secretName: postgres-credentials
    qdrant:
      schedule: "0 3 * * *"  # Daily at 3 AM
      host: qdrant
      port: "6333"
    openwebui:
      schedule: "0 4 * * *"  # Daily at 4 AM
    models:
      enabled: true
      schedule: "0 2 * * *"  # Daily at 2 AM
      remote:
        enabled: false
        url: ""
  
  # Backup verification
  verification:
    enabled: true
    schedule: "0 6 * * 0"  # Weekly on Sunday 6 AM
    pushgatewayUrl: ""