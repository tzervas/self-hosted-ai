# Resource Quotas Helm Chart
# Namespace-level resource limits for homelab cluster
# Optimized for:
#   - homelab: Dual E5-2660v4 (28c/56t), 120GB RAM
#   - Total cluster budget: ~24 CPU cores, 80GB RAM available for workloads

# AI Services namespace - primary AI workloads
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: ai-services-quota
  namespace: ai-services
spec:
  hard:
    # Compute limits - optimized for homelab (reduced Ollama CPU, LiteLLM, Open WebUI, Qdrant)
    requests.cpu: "12"  # Reduced from 16 (actual usage ~3-5 cores)
    requests.memory: 40Gi  # Reduced from 48Gi
    limits.cpu: "18"    # Reduced from 24 (homelab optimization, ~5.5 cores saved)
    limits.memory: 56Gi  # Reduced from 64Gi
    # Pod limits
    pods: "30"
    # Storage
    requests.storage: 500Gi
    persistentvolumeclaims: "20"
    # Services
    services: "20"
    services.loadbalancers: "0"
    services.nodeports: "5"
---
# GPU Workloads namespace - GPU-related services on akula-prime (48GB RAM, RTX 5080 16GB)
# Optimized for homelab: GPU does heavy lifting, CPU minimal
apiVersion: v1
kind: ResourceQuota
metadata:
  name: gpu-workloads-quota
  namespace: gpu-workloads
spec:
  hard:
    # Compute - akula-prime has Intel 14700K (20 cores / 28 threads) + 48GB RAM
    # Reduced from 32â†’12 cores: GPU workloads optimized (GPU does 95% of work)
    requests.cpu: "8"   # Reduced from 20 (realistic for GPU workloads)
    requests.memory: 20Gi  # Reduced from 40Gi (GPU uses VRAM, not RAM)
    limits.cpu: "12"    # Reduced from 32 (homelab optimization, 60% headroom)
    limits.memory: 48Gi  # Reduced from 128Gi (matches akula-prime physical RAM)
    # Pods - Ollama, ComfyUI, TTS, Audio
    pods: "20"
    # Storage - 600Gi to accommodate Ollama models + ComfyUI checkpoints
    requests.storage: 600Gi
    persistentvolumeclaims: "15"
---
# Self-hosted AI namespace - infrastructure services
apiVersion: v1
kind: ResourceQuota
metadata:
  name: self-hosted-ai-quota
  namespace: self-hosted-ai
spec:
  hard:
    requests.cpu: "8"
    requests.memory: 16Gi
    limits.cpu: "12"
    limits.memory: 24Gi
    pods: "20"
    requests.storage: 200Gi
    persistentvolumeclaims: "15"
---
# ARC Runners namespace - CI/CD runners
apiVersion: v1
kind: ResourceQuota
metadata:
  name: arc-runners-quota
  namespace: arc-runners
spec:
  hard:
    # Runners can burst but have upper limits
    requests.cpu: "8"
    requests.memory: 16Gi
    limits.cpu: "20"  # Allow burst for parallel builds
    limits.memory: 40Gi
    pods: "12"  # Max concurrent runners
    requests.storage: 100Gi
---
# GitLab Runners namespace - GitLab CI runners
apiVersion: v1
kind: ResourceQuota
metadata:
  name: gitlab-runners-quota
  namespace: gitlab-runners
spec:
  hard:
    requests.cpu: "8"
    requests.memory: 16Gi
    limits.cpu: "16"
    limits.memory: 32Gi
    pods: "10"
    requests.storage: 100Gi
---
# Monitoring namespace
apiVersion: v1
kind: ResourceQuota
metadata:
  name: monitoring-quota
  namespace: monitoring
spec:
  hard:
    requests.cpu: "6"
    requests.memory: 12Gi
    limits.cpu: "12"
    limits.memory: 24Gi
    pods: "30"
    requests.storage: 100Gi
---
# Linkerd namespace - service mesh
apiVersion: v1
kind: ResourceQuota
metadata:
  name: linkerd-quota
  namespace: linkerd
spec:
  hard:
    requests.cpu: "4"
    requests.memory: 4Gi
    limits.cpu: "8"
    limits.memory: 8Gi
    pods: "15"
---
# Linkerd Viz namespace
apiVersion: v1
kind: ResourceQuota
metadata:
  name: linkerd-viz-quota
  namespace: linkerd-viz
spec:
  hard:
    requests.cpu: "2"
    requests.memory: 2Gi
    limits.cpu: "4"
    limits.memory: 4Gi
    pods: "10"
