# Default values for self-hosted-ai-gpu-worker
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# Create local storage PV/PVC for akula-prime (only needed once)
createLocalStorage: false

# Ollama GPU configuration
ollama:
  gpu:
    enabled: true
    image:
      repository: ollama/ollama
      tag: 0.16.0
      pullPolicy: IfNotPresent
    service:
      type: ClusterIP
      port: 11434
      annotations: {}
    env:
      - name: OLLAMA_NUM_PARALLEL
        value: "4"
      - name: OLLAMA_MAX_LOADED_MODELS
        value: "2"
      - name: OLLAMA_KEEP_ALIVE
        value: "30m"
      - name: OLLAMA_NUM_GPU
        value: "1"
      - name: OLLAMA_KV_CACHE_TYPE
        value: "q8_0"
      - name: OLLAMA_GPU_OVERHEAD
        value: "536870912"  # 512MB reserved
      - name: OLLAMA_FLASH_ATTENTION
        value: "1"
    persistence:
      enabled: true
      size: 100Gi
      storageClass: ""
      accessModes:
        - ReadWriteOnce
      annotations: {}
    nodeSelector:
      gpu: "true"
    tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
    resources:
      requests:
        nvidia.com/gpu: 1
        memory: "8Gi"
        cpu: "2000m"
      limits:
        nvidia.com/gpu: 1
        memory: "32Gi"
        cpu: "8000m"

# ComfyUI configuration (custom Blackwell image with PyTorch 2.7+cu128)
# Enabled via ArgoCD app override; uses GPU time-slicing alongside ollama-gpu
comfyui:
  enabled: false
  image:
    repository: ghcr.io/tzervas/comfyui-blackwell
    tag: latest
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 8188  # Direct ComfyUI listener (no proxy)
    annotations: {}
  env: []
  persistence:
    enabled: true
    size: 50Gi
    storageClass: ""
    accessModes:
      - ReadWriteOnce
    annotations: {}
  nodeSelector:
    gpu: "true"
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  resources:
    requests:
      nvidia.com/gpu: 1
      memory: "4Gi"
      cpu: "1000m"
    limits:
      nvidia.com/gpu: 1
      memory: "16Gi"
      cpu: "4000m"

# Automatic1111 Stable Diffusion WebUI configuration
# NOTE: Disabled due to single GPU constraint. Requires GPU time-slicing to run alongside ollama-gpu.
automatic1111:
  enabled: false
  image:
    repository: automatic1111/stable-diffusion-webui
    tag: latest
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 7860
    annotations: {}
  env:
    - name: CLI_ARGS
      value: "--listen --port 7860 --api --medvram --no-half-vae --xformers"
    - name: COMMANDLINE_ARGS
      value: "--listen --port 7860 --api --medvram --no-half-vae --xformers"
  persistence:
    enabled: true
    size: 100Gi
    storageClass: ""
    accessModes:
      - ReadWriteOnce
    annotations: {}
  nodeSelector:
    gpu: "true"
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  resources:
    requests:
      nvidia.com/gpu: 1
      memory: "8Gi"
      cpu: "2000m"
    limits:
      nvidia.com/gpu: 1
      memory: "24Gi"
      cpu: "6000m"

# TTS Server (XTTS-v2) configuration
tts:
  enabled: true
  image:
    repository: ghcr.io/coqui-ai/tts
    tag: latest
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 5002
    annotations: {}
  env:
    - name: USE_CUDA
      value: "true"
    - name: COQUI_TOS_AGREED
      value: "1"
  nodeSelector:
    kubernetes.io/hostname: akula-prime
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  resources:
    requests:
      memory: "2Gi"
      cpu: "500m"
    limits:
      memory: "8Gi"
      cpu: "2000m"

# Audio Generation Server (AudioLDM2/MusicGen/Bark) configuration
audio:
  enabled: false  # Disabled by default; enable explicitly in ArgoCD app
  image:
    repository: ghcr.io/tzervas/audio-server
    tag: "1.1.0"
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 5004
    annotations: {}
  env:
    - name: HF_HOME
      value: "/models/huggingface"
    - name: TORCH_HOME
      value: "/models/torch"
  nodeSelector:
    kubernetes.io/hostname: akula-prime
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  resources:
    requests:
      memory: "4Gi"
      cpu: "1000m"
    limits:
      memory: "12Gi"
      cpu: "4000m"

# Video Generation Server (AnimateDiff-Lightning/SVD) configuration
video:
  enabled: false  # Disabled by default; enable explicitly in ArgoCD app
  image:
    repository: ghcr.io/tzervas/video-server
    tag: "1.0.0"
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 5005
    annotations: {}
  env:
    - name: HF_HOME
      value: "/models/huggingface"
    - name: TORCH_HOME
      value: "/models/torch"
  nodeSelector:
    kubernetes.io/hostname: akula-prime
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  resources:
    requests:
      memory: "6Gi"
      cpu: "1000m"
    limits:
      memory: "14Gi"
      cpu: "4000m"