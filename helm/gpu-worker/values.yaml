# Default values for self-hosted-ai-gpu-worker
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# Global security contexts for Pod Security Standards "baseline" compliance
# Note: Using "baseline" instead of "restricted" due to upstream image limitations
# Some GPU container images (ollama, tts) are not built for non-root execution
# Applied to all pods/containers unless overridden per-service
podSecurityContext:
  runAsNonRoot: false  # Allow root for compatibility (PSS baseline)
  seccompProfile:
    type: RuntimeDefault

containerSecurityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
      - ALL
  readOnlyRootFilesystem: false  # GPU apps need writable /tmp, model caches

# Create local storage PV/PVC for akula-prime (only needed once)
createLocalStorage: false

# Ollama GPU configuration
ollama:
  gpu:
    enabled: true
    image:
      repository: ollama/ollama
      tag: 0.16.0
      pullPolicy: IfNotPresent
    service:
      type: ClusterIP
      port: 11434
      annotations: {}
    env:
      - name: OLLAMA_NUM_PARALLEL
        value: "4"
      - name: OLLAMA_MAX_LOADED_MODELS
        value: "2"
      - name: OLLAMA_KEEP_ALIVE
        value: "30m"
      - name: OLLAMA_NUM_GPU
        value: "1"
      - name: OLLAMA_KV_CACHE_TYPE
        value: "q8_0"
      - name: OLLAMA_GPU_OVERHEAD
        value: "536870912"  # 512MB reserved
      - name: OLLAMA_FLASH_ATTENTION
        value: "1"
    persistence:
      enabled: true
      size: 100Gi
      storageClass: ""
      accessModes:
        - ReadWriteOnce
      annotations: {}
    nodeSelector:
      gpu: "true"
    tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
    resources:
      requests:
        nvidia.com/gpu: 1
        memory: "8Gi"
        cpu: "500m"  # GPU does inference, CPU minimal
      limits:
        nvidia.com/gpu: 1
        memory: "32Gi"
        cpu: "2000m"  # Reduced from 8000m (homelab optimization)

# ComfyUI configuration (custom Blackwell image with PyTorch 2.7+cu128)
# Enabled via ArgoCD app override; uses GPU time-slicing alongside ollama-gpu
comfyui:
  enabled: false
  image:
    repository: ghcr.io/tzervas/comfyui-blackwell
    tag: "latest"  # Latest build with ComfyUI + Manager + PyTorch 2.7+cu128 + CUDA 12.8
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 8188  # Direct ComfyUI listener (no proxy)
    annotations: {}
  env:
    # VRAM optimization for 16GB GPU (RTX 5080)
    # lowvram: model offloading to RAM when not in use
    # preview-method auto: optimized preview generation
    - name: COMMANDLINE_ARGS
      value: "--lowvram --preview-method auto"
    # PyTorch CUDA memory optimization
    - name: PYTORCH_CUDA_ALLOC_CONF
      value: "max_split_size_mb:512"
  persistence:
    enabled: true
    size: 50Gi
    storageClass: ""
    accessModes:
      - ReadWriteOnce
    annotations: {}
  # Models storage configuration
  models:
    # Use existing PVC for models (shared across GPU workloads)
    existingClaim: ""  # If set, uses this PVC instead of creating one
    # Storage size/class only used if existingClaim is empty
    size: 500Gi
    storageClass: ""
  nodeSelector:
    gpu: "true"
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  resources:
    requests:
      nvidia.com/gpu: 1
      memory: "4Gi"
      cpu: "500m"  # GPU does video gen, CPU minimal
    limits:
      nvidia.com/gpu: 1
      memory: "16Gi"
      cpu: "1000m"  # Reduced from 4000m (homelab optimization)

# Automatic1111 Stable Diffusion WebUI configuration
# NOTE: Disabled due to single GPU constraint. Requires GPU time-slicing to run alongside ollama-gpu.
automatic1111:
  enabled: false
  image:
    repository: automatic1111/stable-diffusion-webui
    tag: latest
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 7860
    annotations: {}
  env:
    - name: CLI_ARGS
      value: "--listen --port 7860 --api --medvram --no-half-vae --xformers"
    - name: COMMANDLINE_ARGS
      value: "--listen --port 7860 --api --medvram --no-half-vae --xformers"
  persistence:
    enabled: true
    size: 100Gi
    storageClass: ""
    accessModes:
      - ReadWriteOnce
    annotations: {}
  nodeSelector:
    gpu: "true"
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  resources:
    requests:
      nvidia.com/gpu: 1
      memory: "8Gi"
      cpu: "500m"  # GPU does image gen, CPU minimal
    limits:
      nvidia.com/gpu: 1
      memory: "24Gi"
      cpu: "1500m"  # Reduced from 6000m (homelab optimization)

# TTS Server (XTTS-v2) configuration
tts:
  enabled: true
  image:
    repository: ghcr.io/coqui-ai/tts
    tag: latest
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 5002
    annotations: {}
  env:
    - name: USE_CUDA
      value: "true"
    - name: COQUI_TOS_AGREED
      value: "1"
  nodeSelector:
    kubernetes.io/hostname: akula-prime
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  resources:
    requests:
      memory: "2Gi"
      cpu: "500m"
    limits:
      memory: "8Gi"
      cpu: "2000m"

# Audio Generation Server (AudioLDM2/MusicGen/Bark) configuration
audio:
  enabled: false  # Disabled by default; enable explicitly in ArgoCD app
  image:
    repository: ghcr.io/tzervas/audio-server
    tag: "1.1.0"
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 5004
    annotations: {}
  env:
    - name: HF_HOME
      value: "/models/huggingface"
    - name: TORCH_HOME
      value: "/models/torch"
  nodeSelector:
    kubernetes.io/hostname: akula-prime
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  resources:
    requests:
      memory: "4Gi"
      cpu: "1000m"
    limits:
      memory: "12Gi"
      cpu: "4000m"

# Video Generation Server (AnimateDiff-Lightning/SVD) configuration
video:
  enabled: false  # Disabled by default; enable explicitly in ArgoCD app
  image:
    repository: ghcr.io/tzervas/video-server
    tag: "1.0.0"
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 5005
    annotations: {}
  env:
    - name: HF_HOME
      value: "/models/huggingface"
    - name: TORCH_HOME
      value: "/models/torch"
  nodeSelector:
    kubernetes.io/hostname: akula-prime
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  resources:
    requests:
      memory: "6Gi"
      cpu: "1000m"
    limits:
      memory: "14Gi"
      cpu: "4000m"