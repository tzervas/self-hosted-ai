# Default values for self-hosted-ai-gpu-worker
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# Ollama GPU configuration
ollama:
  gpu:
    enabled: true
    image:
      repository: ollama/ollama
      tag: 0.16.0
      pullPolicy: IfNotPresent
    service:
      type: ClusterIP
      port: 11434
      annotations: {}
    env:
      - name: OLLAMA_NUM_PARALLEL
        value: "4"
      - name: OLLAMA_MAX_LOADED_MODELS
        value: "2"
      - name: OLLAMA_KEEP_ALIVE
        value: "30m"
      - name: OLLAMA_NUM_GPU
        value: "1"
      - name: OLLAMA_KV_CACHE_TYPE
        value: "q8_0"
      - name: OLLAMA_GPU_OVERHEAD
        value: "536870912"  # 512MB reserved
      - name: OLLAMA_FLASH_ATTENTION
        value: "1"
    persistence:
      enabled: true
      size: 100Gi
      storageClass: ""
      accessModes:
        - ReadWriteOnce
      annotations: {}
    nodeSelector:
      gpu: "true"
    tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
    resources:
      requests:
        nvidia.com/gpu: 1
        memory: "8Gi"
        cpu: "2000m"
      limits:
        nvidia.com/gpu: 1
        memory: "32Gi"
        cpu: "8000m"

# ComfyUI configuration
comfyui:
  enabled: true
  image:
    repository: ghcr.io/yolandawu/comfyui
    tag: latest
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 8188
    annotations: {}
  env:
    - name: CLI_ARGS
      value: "--listen 0.0.0.0 --port 8188"
  persistence:
    enabled: true
    size: 50Gi
    storageClass: ""
    accessModes:
      - ReadWriteOnce
    annotations: {}
  nodeSelector:
    gpu: "true"
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  resources:
    requests:
      nvidia.com/gpu: 1
      memory: "4Gi"
      cpu: "1000m"
    limits:
      nvidia.com/gpu: 1
      memory: "16Gi"
      cpu: "4000m"

# Automatic1111 Stable Diffusion WebUI configuration
automatic1111:
  enabled: true
  image:
    repository: automatic1111/stable-diffusion-webui
    tag: latest
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 7860
    annotations: {}
  env:
    - name: CLI_ARGS
      value: "--listen --port 7860 --api --medvram --no-half-vae --xformers"
    - name: COMMANDLINE_ARGS
      value: "--listen --port 7860 --api --medvram --no-half-vae --xformers"
  persistence:
    enabled: true
    size: 100Gi
    storageClass: ""
    accessModes:
      - ReadWriteOnce
    annotations: {}
  nodeSelector:
    gpu: "true"
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  resources:
    requests:
      nvidia.com/gpu: 1
      memory: "8Gi"
      cpu: "2000m"
    limits:
      nvidia.com/gpu: 1
      memory: "24Gi"
      cpu: "6000m"

# TTS Server (XTTS-v2) configuration
tts:
  enabled: true
  image:
    repository: ghcr.io/coqui-ai/tts
    tag: latest
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 5002
    annotations: {}
  env:
    - name: USE_CUDA
      value: "true"
    - name: COQUI_TOS_AGREED
      value: "1"
  nodeSelector:
    kubernetes.io/hostname: akula-prime
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  resources:
    requests:
      memory: "2Gi"
      cpu: "500m"
    limits:
      memory: "8Gi"
      cpu: "2000m"

# Audio Generation Server (AudioLDM2/MusicGen) configuration
audio:
  enabled: true
  image:
    repository: ghcr.io/huggingface/text-generation-inference
    tag: latest
    pullPolicy: IfNotPresent
  service:
    type: ClusterIP
    port: 5004
    annotations: {}
  env:
    - name: MODEL_ID
      value: "facebook/musicgen-small"
    - name: AUDIO_MODEL
      value: "audioldm2-large"
  nodeSelector:
    kubernetes.io/hostname: akula-prime
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  resources:
    requests:
      memory: "4Gi"
      cpu: "1000m"
    limits:
      memory: "12Gi"
      cpu: "4000m"