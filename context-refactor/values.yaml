# =============================================================================
# Ollama Helm Values - CORRECTED for RTX 5080 (16GB VRAM)
# =============================================================================
# Chart: otwld/ollama
# Repository: https://helm.otwld.com (NOT otwld.github.io - migrated)
# Version: 0.24.x
# =============================================================================
# CRITICAL: Original plan referenced non-existent models:
# - qwen2.5-coder:72b-instruct-q4_k_m → DOES NOT EXIST (max is 32B)
# - deepseek-coder-v2:33b-instruct-q5_k_m → DOES NOT EXIST (V2 is 16B or 236B only)
# This file uses VERIFIED models that fit in 16GB VRAM
# =============================================================================

replicaCount: 1

image:
  repository: ollama/ollama
  pullPolicy: IfNotPresent
  # tag: latest  # Use default, includes CUDA support

# GPU Configuration
ollama:
  gpu:
    enabled: true
    type: 'nvidia'
    number: 1  # Requests 1 time-sliced GPU replica (4 available from RTX 5080)

  # ==========================================================================
  # MODEL PRELOADING - VERIFIED TO FIT IN 16GB VRAM
  # ==========================================================================
  models:
    # Models to pull on startup
    pull:
      # Primary reasoning model - 8B fits comfortably (~5GB)
      - llama3.2:8b
      # Code generation - 13B is tight but works (~8GB)
      - codellama:13b
      # Fast autocomplete - 7B minimal footprint (~4GB)
      - qwen2.5-coder:7b-instruct
      # Embeddings - tiny footprint (~274MB)
      - nomic-embed-text
      
    # Model to run immediately on startup (keeps warm)
    run:
      - llama3.2:8b
      
    # Custom Modelfiles - create specialized variants
    create:
      # Primary spec-driven coding assistant
      - name: coding-assistant
        template: |
          FROM llama3.2:8b
          PARAMETER num_ctx 32768
          PARAMETER temperature 0.3
          PARAMETER repeat_penalty 1.1
          PARAMETER top_p 0.9
          SYSTEM """
          You are a rigorous, spec-driven software engineer. Your responses must:
          
          1. ALWAYS reference specifications before writing code
          2. Generate comprehensive tests BEFORE implementation
          3. Cite knowledge base sources using [N] notation for any external patterns
          4. Structure output as: Reasoning → Tests → Code → Self-check
          5. Score confidence 1-10; if <8, suggest improvements
          6. Use structured JSON for deliverables when requested
          
          Languages: Python (PEP8, Black formatting), Rust (idiomatic), Go, TypeScript
          Principles: SOLID, DRY, KISS, YAGNI, composition over inheritance
          Testing: pytest, cargo test, comprehensive coverage
          """
          
      # DevOps/Infrastructure specialist
      - name: devops-specialist
        template: |
          FROM llama3.2:8b
          PARAMETER num_ctx 16384
          PARAMETER temperature 0.4
          SYSTEM """
          You are a senior DevOps engineer and Kubernetes specialist.
          
          Core expertise:
          - Kubernetes architecture, Helm charts, ArgoCD GitOps
          - GitLab CI/CD pipelines, multi-architecture builds
          - Infrastructure as Code (Terraform, Pulumi)
          - Container security and hardening
          - Observability (Prometheus, Grafana, Loki)
          
          When generating Kubernetes manifests:
          - Include resource limits and requests
          - Use pod security contexts (runAsNonRoot, drop ALL capabilities)
          - Add health checks (liveness, readiness probes)
          - Include appropriate labels and annotations
          - Consider network policies for isolation
          - Follow GitOps principles (declarative, version controlled)
          """
          
      # Fast code completion assistant
      - name: code-complete
        template: |
          FROM qwen2.5-coder:7b-instruct
          PARAMETER num_ctx 8192
          PARAMETER temperature 0.2
          PARAMETER repeat_penalty 1.0
          SYSTEM """
          You are a fast, precise code completion assistant.
          Complete code snippets efficiently.
          Prioritize speed and accuracy.
          Follow existing code style.
          """

# Persistent storage for models (~200GB recommended)
persistentVolume:
  enabled: true
  accessModes:
    - ReadWriteOnce
  size: 200Gi
  storageClass: longhorn  # Or "local-path" for k3s default

# Resource allocation (fits on 16GB RAM node alongside other services)
resources:
  requests:
    memory: "8Gi"
    cpu: "2"
    nvidia.com/gpu: "1"  # Time-sliced GPU
  limits:
    memory: "14Gi"  # Leave headroom for system
    cpu: "4"
    nvidia.com/gpu: "1"

# Service configuration
service:
  type: ClusterIP
  port: 11434

# Ingress for external access
ingress:
  enabled: true
  className: "traefik"  # k3s default
  annotations:
    traefik.ingress.kubernetes.io/router.entrypoints: websecure
    traefik.ingress.kubernetes.io/router.tls: "true"
  hosts:
    - host: ollama.homelab.local
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: ollama-tls
      hosts:
        - ollama.homelab.local

# Health checks
livenessProbe:
  enabled: true
  path: /
  initialDelaySeconds: 120  # Models take time to load
  periodSeconds: 15
  timeoutSeconds: 10
  failureThreshold: 6

readinessProbe:
  enabled: true
  path: /api/tags
  initialDelaySeconds: 60
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

# NVIDIA runtime class (required for GPU)
runtimeClassName: nvidia

# Node selection - target GPU node
nodeSelector:
  nvidia.com/gpu: "present"

# Tolerations for GPU taint
tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule

# Environment variables for performance tuning
extraEnv:
  # Bind to all interfaces for cluster access
  - name: OLLAMA_HOST
    value: "0.0.0.0:11434"
  # Concurrent request handling (limited by VRAM)
  - name: OLLAMA_NUM_PARALLEL
    value: "2"
  # Maximum models in VRAM simultaneously
  - name: OLLAMA_MAX_LOADED_MODELS
    value: "2"
  # Keep primary model loaded (reduces swap latency)
  - name: OLLAMA_KEEP_ALIVE
    value: "24h"
  # Debug logging (disable in production)
  - name: OLLAMA_DEBUG
    value: "false"
  # Force CUDA device (helpful for Blackwell/RTX 50 series)
  - name: CUDA_VISIBLE_DEVICES
    value: "0"

# Pod annotations
podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "11434"
  prometheus.io/path: "/metrics"

# Security context
podSecurityContext:
  runAsNonRoot: false  # Ollama needs root for GPU access currently
  fsGroup: 1000

# Init container to verify GPU access
initContainers: []
# Uncomment to add GPU verification
# - name: verify-gpu
#   image: nvidia/cuda:12.8-base-ubuntu22.04
#   command: ["nvidia-smi"]

# ==========================================================================
# MODEL SIZE REFERENCE (for planning)
# ==========================================================================
# Model                          | Parameters | VRAM (Q4) | Notes
# -------------------------------|------------|-----------|------------------
# llama3.2:8b                    | 8B         | ~5GB      | ✅ Primary
# codellama:13b                  | 13B        | ~8GB      | ✅ Code gen
# qwen2.5-coder:7b-instruct      | 7B         | ~4GB      | ✅ Autocomplete
# nomic-embed-text               | 137M       | ~274MB    | ✅ Embeddings
# mistral:7b-instruct            | 7B         | ~4GB      | ✅ Alternative
# llava:13b                      | 13B        | ~8GB      | ⚠️ Vision (tight)
# qwen2.5-coder:14b-instruct     | 14B        | ~9GB      | ⚠️ Larger coder
# -------------------------------|------------|-----------|------------------
# qwen2.5-coder:32b-instruct     | 32B        | ~20GB     | ❌ TOO LARGE
# deepseek-coder-v2:16b          | 16B        | ~10GB     | ⚠️ Very tight
# ANY 70B+ model                 | 70B+       | ~40GB+    | ❌ IMPOSSIBLE
# ==========================================================================
