# Extended Multi-Modal Services
# Audio, Video, and Advanced AI Processing
# Deploy with: docker compose -f docker-compose.yml -f docker-compose.multimodal.yml up -d

services:
  # === AUDIO SERVICES ===
  
  # Whisper - Speech-to-Text (OpenAI Whisper)
  whisper:
    image: onerahmet/openai-whisper-asr-webservice:latest-gpu
    container_name: whisper-stt
    ports:
      - "${WHISPER_PORT:-9000}:9000"
    environment:
      - ASR_MODEL=${WHISPER_MODEL:-base}
      - ASR_ENGINE=openai_whisper
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    volumes:
      - ${DATA_PATH:-/data}/whisper:/root/.cache
    restart: unless-stopped
    profiles:
      - multimodal
      - audio

  # Piper TTS - Text-to-Speech
  piper-tts:
    image: rhasspy/piper:1.2.0
    container_name: piper-tts
    ports:
      - "${PIPER_PORT:-5000}:5000"
    volumes:
      - ${DATA_PATH:-/data}/piper/voices:/voices
      - ${DATA_PATH:-/data}/piper/cache:/cache
    command: --port 5000 --voices-dir /voices
    restart: unless-stopped
    profiles:
      - multimodal
      - audio

  # Coqui TTS Community - Advanced Text-to-Speech with voice cloning
  # Note: Official Coqui AI project archived; using community-maintained fork
  coqui-tts:
    image: ghcr.io/coqui-ai-community/tts:latest
    container_name: coqui-tts
    ports:
      - "${COQUI_PORT:-5002}:5002"
    environment:
      - TTS_MODEL=${TTS_MODEL:-tts_models/en/ljspeech/tacotron2-DDC}
    volumes:
      - ${DATA_PATH:-/data}/coqui:/root/.local/share/tts
    command: --port 5002 --use_cuda true
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    profiles:
      - multimodal
      - audio

  # === VECTOR DATABASE & EMBEDDINGS ===
  
  # Qdrant - Vector database for embeddings
  qdrant:
    image: qdrant/qdrant:v1.12.1
    container_name: qdrant-vector-db
    ports:
      - "${QDRANT_PORT:-6333}:6333"
      - "${QDRANT_GRPC_PORT:-6334}:6334"
    volumes:
      - ${DATA_PATH:-/data}/qdrant/storage:/qdrant/storage
      - ${DATA_PATH:-/data}/qdrant/snapshots:/qdrant/snapshots
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__LOG_LEVEL=${QDRANT_LOG_LEVEL:-INFO}
    restart: unless-stopped
    profiles:
      - multimodal
      - embeddings
      - full

  # === VIDEO PROCESSING ===
  
  # FFmpeg service for video processing
  ffmpeg-service:
    image: jrottenberg/ffmpeg:7.1-alpine
    container_name: ffmpeg-processor
    volumes:
      - ${DATA_PATH:-/data}/video/input:/input
      - ${DATA_PATH:-/data}/video/output:/output
      - ${DATA_PATH:-/data}/video/temp:/temp
    entrypoint: /bin/sh
    command: -c "while true; do sleep 3600; done"
    restart: unless-stopped
    profiles:
      - multimodal
      - video

  # === DOCUMENT PROCESSING ===
  
  # Tika - Document parsing and content extraction
  tika:
    image: apache/tika:3.0.0-full
    container_name: tika-parser
    ports:
      - "${TIKA_PORT:-9998}:9998"
    environment:
      - TIKA_CONFIG=/config/tika-config.xml
    restart: unless-stopped
    profiles:
      - multimodal
      - documents

  # === API GATEWAY & ORCHESTRATION ===
  
  # LiteLLM Proxy - Unified API gateway for all models with GPU queue
  litellm:
    image: ghcr.io/berriai/litellm:v1.80.11-stable
    container_name: litellm-proxy
    ports:
      - "${LITELLM_PORT:-4000}:4000"
      - "${LITELLM_METRICS_PORT:-9091}:9090"
    environment:
      - "DATABASE_URL=postgresql://${POSTGRES_USER:-litellm}:${POSTGRES_PASSWORD}@postgres:5432/litellm"
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}
      - OLLAMA_CPU_URL=http://ollama-cpu:11434
      - OLLAMA_GPU_URL=http://${GPU_WORKER_HOST:-192.168.1.99}:11434
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - LANGFUSE_ENABLED=${LANGFUSE_ENABLED:-false}
      - LANGFUSE_PUBLIC_KEY=${LANGFUSE_PUBLIC_KEY:-}
      - LANGFUSE_SECRET_KEY=${LANGFUSE_SECRET_KEY:-}
      - LANGFUSE_HOST=${LANGFUSE_HOST:-https://cloud.langfuse.com}
    volumes:
      - ../config/litellm-config.yml:/app/config.yaml:ro
    command: --config /app/config.yaml --port 4000
    depends_on:
      - postgres
      - redis
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    profiles:
      - full
      - api-gateway

  # Automatic1111 Stable Diffusion WebUI - GPU-based image generation
  # Alternative to ComfyUI with user-friendly interface and extensive extensions
  # automatic1111:
  #   image: ghcr.io/automatic1111/stable-diffusion-webui:latest
  #   container_name: automatic1111-server
  #   ports:
  #     - "${AUTOMATIC1111_PORT:-7860}:7860"
  #   volumes:
  #     # Shared model storage (mount from host or shared volume)
  #     - ${DATA_PATH:-/data}/models:/app/stable-diffusion-webui/models
  #     # A1111-specific outputs and configurations
  #     - ${DATA_PATH:-/data}/automatic1111/outputs:/app/stable-diffusion-webui/outputs
  #     - ${DATA_PATH:-/data}/automatic1111/configs:/app/stable-diffusion-webui/configs
  #     - ${DATA_PATH:-/data}/automatic1111/extensions:/app/stable-diffusion-webui/extensions
  #   environment:
  #     # A1111 server settings optimized for server deployment
  #     - COMMANDLINE_ARGS=${A1111_CLI_ARGS:---listen --port 7860 --api --medvram --no-half-vae --xformers --enable-insecure-extension-access --skip-torch-cuda-test}
  #     - WEBUI_RESTARTING_TIMEOUT=0
  #     # Disable authentication for internal access
  #     - WEBUI_AUTH=
  #     # GPU optimization
  #     - TORCH_COMMAND=cache
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #         - driver: nvidia
  #           count: 1
  #           capabilities: [gpu]
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:7860/"]
  #     interval: 60s
  #     timeout: 30s
  #     retries: 3
  #     start_period: 180s
  #   restart: unless-stopped
  #   profiles:
  #     - full
  #     - image-generation

  # Image Generation Router - Routes requests between ComfyUI and A1111
  # Provides intelligent load balancing and backend selection
  image-router:
    build:
      context: ../image-router
      dockerfile: Dockerfile
    container_name: image-router
    ports:
      - "${IMAGE_ROUTER_PORT:-8000}:8000"
    environment:
      # Backend URLs (will be updated when deployed)
      - COMFYUI_URL=http://comfyui:8188
      # - AUTOMATIC1111_URL=http://automatic1111:7860
    # depends_on:
    #   - automatic1111
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    profiles:
      - full
      - image-generation

  # PostgreSQL 16 for LiteLLM and metadata storage (current production)
  postgres:
    image: postgres:16-alpine
    container_name: postgres-db
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    environment:
      - POSTGRES_DB=${POSTGRES_DB:-litellm}
      - POSTGRES_USER=${POSTGRES_USER:-litellm}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - PGDATA=/var/lib/postgresql/data/pgdata
    volumes:
      - ${DATA_PATH:-/data}/postgres:/var/lib/postgresql/data
    restart: unless-stopped
    profiles:
      - full
      - api-gateway

  # PostgreSQL 17 (new version for parallel deployment)
  # To migrate: 1) Deploy postgres-17, 2) Run migration script, 3) Update litellm to use postgres-17, 4) Remove postgres service
  postgres-17:
    image: postgres:17.2-alpine
    container_name: postgres-17-db
    ports:
      - "${POSTGRES_17_PORT:-5433}:5432"
    environment:
      - POSTGRES_DB=${POSTGRES_DB:-litellm}
      - POSTGRES_USER=${POSTGRES_USER:-litellm}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - PGDATA=/var/lib/postgresql/data/pgdata
    volumes:
      - ${DATA_PATH:-/data}/postgres-17:/var/lib/postgresql/data
    restart: unless-stopped
    profiles:
      - postgres-migration

  # === WORKFLOW ORCHESTRATION ===
  
  # n8n - Workflow automation and orchestration
  n8n:
    image: n8nio/n8n:1.68.0
    container_name: n8n-workflows
    ports:
      - "${N8N_PORT:-5678}:5678"
    environment:
      - N8N_BASIC_AUTH_ACTIVE=true
      - N8N_BASIC_AUTH_USER=${N8N_USER:-admin}
      - N8N_BASIC_AUTH_PASSWORD=${N8N_PASSWORD}
      - N8N_HOST=${N8N_HOST:-0.0.0.0}
      - N8N_PORT=5678
      - N8N_PROTOCOL=${N8N_PROTOCOL:-http}
      - NODE_ENV=production
      - WEBHOOK_URL=${N8N_WEBHOOK_URL:-http://localhost:5678}
      - GENERIC_TIMEZONE=${TZ:-America/New_York}
    volumes:
      - ${DATA_PATH:-/data}/n8n:/home/node/.n8n
    restart: unless-stopped
    profiles:
      - full
      - orchestration

  # === MONITORING & METRICS ===
  
  # Grafana Loki - Log aggregation (v3.x with new storage format)
  loki:
    image: grafana/loki:3.3.2
    container_name: loki-logs
    ports:
      - "${LOKI_PORT:-3100}:3100"
    volumes:
      - ${DATA_PATH:-/data}/loki:/loki
      - ./monitoring/loki-config.yml:/etc/loki/local-config.yaml
    command: -config.file=/etc/loki/local-config.yaml
    restart: unless-stopped
    profiles:
      - monitoring
      - full

  # Promtail - Log collector for Loki (v3.x)
  promtail:
    image: grafana/promtail:3.3.2
    container_name: promtail-collector
    volumes:
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - ./monitoring/promtail-config.yml:/etc/promtail/config.yml
    command: -config.file=/etc/promtail/config.yml
    depends_on:
      - loki
    restart: unless-stopped
    profiles:
      - monitoring
      - full
