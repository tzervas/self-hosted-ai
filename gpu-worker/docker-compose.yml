services:
  ollama-gpu:
    image: ollama/ollama:0.13.5
    container_name: ollama-gpu-worker
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    volumes:
      - ${DATA_PATH:-/data}/ollama-gpu:/root/.ollama
    environment:
      - OLLAMA_NUM_PARALLEL=${OLLAMA_NUM_PARALLEL:-4}
      - OLLAMA_MAX_LOADED_MODELS=${OLLAMA_MAX_LOADED_MODELS:-2}
      - OLLAMA_KEEP_ALIVE=${OLLAMA_KEEP_ALIVE:-30m}
      - OLLAMA_GPU_LAYERS=${OLLAMA_GPU_LAYERS:-99}
      - OLLAMA_FLASH_ATTENTION=${OLLAMA_FLASH_ATTENTION:-1}
      - OLLAMA_HOST=${OLLAMA_HOST:-0.0.0.0}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

  # GPU Resource Manager - Dynamic GPU allocation
  gpu-manager:
    build:
      context: ../gpu-manager
      dockerfile: Dockerfile
    container_name: gpu-manager
    ports:
      - "${GPU_MANAGER_PORT:-8100}:8100"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8100/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # ComfyUI - GPU-accelerated image generation
  # Integrates with Open WebUI for text-to-image workflows
  comfyui:
    image: ghcr.io/ai-dock/comfyui:v2-cuda-12.6.3-ubuntu22.04
    container_name: comfyui-worker
    ports:
      - "${COMFYUI_PORT:-8188}:8188"
    volumes:
      # Persistent storage for models, outputs, and customizations
      - ${DATA_PATH:-/data}/comfyui/models:/opt/ComfyUI/models
      - ${DATA_PATH:-/data}/comfyui/output:/opt/ComfyUI/output
      - ${DATA_PATH:-/data}/comfyui/input:/opt/ComfyUI/input
      - ${DATA_PATH:-/data}/comfyui/custom_nodes:/opt/ComfyUI/custom_nodes
      # Mount workflow configurations (read-only)
      - ${DATA_PATH:-/data}/comfyui/workflows:/opt/ComfyUI/user/default/workflows:ro
    environment:
      # ComfyUI server settings
      - COMFYUI_PORT=8188
      - CLI_ARGS=${COMFYUI_CLI_ARGS:---listen 0.0.0.0 --preview-method auto}
      # Disable provisioning scripts (we manage models ourselves)
      - PROVISIONING_SCRIPT=
      - CF_QUICK_TUNNELS=false
      - WEB_ENABLE_AUTH=false
      - WEB_USER=${COMFYUI_WEB_USER:-admin}
      - WEB_PASSWORD=${COMFYUI_WEB_PASSWORD:-}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8188/system_stats"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped

  # Automatic1111 Stable Diffusion WebUI - Alternative image generation interface
  # Provides user-friendly web interface with extensive features and extensions
  automatic1111:
    image: automatic1111/stable-diffusion-webui:latest
    container_name: automatic1111-worker
    ports:
      - "${AUTOMATIC1111_PORT:-7860}:7860"
    volumes:
      # Shared model storage with ComfyUI
      - ${DATA_PATH:-/data}/models:/app/stable-diffusion-webui/models
      # A1111-specific outputs and configurations
      - ${DATA_PATH:-/data}/automatic1111/outputs:/app/stable-diffusion-webui/outputs
      - ${DATA_PATH:-/data}/automatic1111/configs:/app/stable-diffusion-webui/configs
      - ${DATA_PATH:-/data}/automatic1111/extensions:/app/stable-diffusion-webui/extensions
    environment:
      # A1111 server settings
      - COMMANDLINE_ARGS=${A1111_CLI_ARGS:---listen --port 7860 --api --medvram --no-half-vae --xformers --enable-insecure-extension-access}
      - WEBUI_RESTARTING_TIMEOUT=0
      # Disable authentication for internal access
      - WEBUI_AUTH=
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7860/"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 120s
    restart: unless-stopped

  # Whisper Service - Audio transcription
  whisper:
    image: onerahmet/openai-whisper-asr-webservice:latest-gpu
    container_name: whisper-worker
    ports:
      - "${WHISPER_PORT:-9000}:9000"
    volumes:
      - ${DATA_PATH:-/data}/whisper:/root/.cache/whisper
    environment:
      - ASR_MODEL=${WHISPER_MODEL:-large-v3}
      - ASR_ENGINE=faster_whisper
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped

networks:
  default:
    name: self-hosted-ai
    driver: bridge