# GPU Worker Environment Configuration
# Copy to .env and customize values
# DO NOT commit .env with real secrets

# =============================================================================
# OLLAMA GPU CONFIGURATION
# =============================================================================
OLLAMA_PORT=11434

# Concurrent requests per model (RTX 5080 can handle 4 easily)
OLLAMA_NUM_PARALLEL=4

# Models kept in VRAM simultaneously (2 for 16GB VRAM)
OLLAMA_MAX_LOADED_MODELS=2

# Time before unloading idle models
OLLAMA_KEEP_ALIVE=30m

# GPU offload layers (99 = all layers to GPU)
OLLAMA_GPU_LAYERS=99

# Enable flash attention for faster inference
OLLAMA_FLASH_ATTENTION=1

# Bind to all interfaces (required for LAN access)
OLLAMA_HOST=0.0.0.0

# =============================================================================
# DATA PERSISTENCE
# =============================================================================
# Path for bind mounts (models stored here)
DATA_PATH=/data

# =============================================================================
# COMFYUI CONFIGURATION
# =============================================================================
# Port for ComfyUI web interface and API
COMFYUI_PORT=8188

# CLI arguments for ComfyUI server
# --listen 0.0.0.0: Accept connections from LAN
# --preview-method auto: Enable live preview during generation
COMFYUI_CLI_ARGS=--listen 0.0.0.0 --preview-method auto

# Optional: Basic auth for ComfyUI web interface
# Leave COMFYUI_WEB_PASSWORD empty to disable auth
COMFYUI_WEB_USER=admin
COMFYUI_WEB_PASSWORD=
